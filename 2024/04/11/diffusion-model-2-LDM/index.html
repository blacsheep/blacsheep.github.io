<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="一开始是看的Stable diffusion的,以为是这里开始讲conditioning mechanisms的,后来发现他们idea是从LDM过来的,那么这次就学习一下LDM,这次没来得及看模型,这两天稍微有点忙,看周末会不会有空补一下了. Latent Diffusion Models LDM区别于我们之前看的一般diffusion model主要在于两点  之前我们对一般扩散模型是直接喂图片">
<meta property="og:type" content="article">
<meta property="og:title" content="diffusion model(2)-LDM">
<meta property="og:url" content="http://example.com/2024/04/11/diffusion-model-2-LDM/index.html">
<meta property="og:site_name" content="blacsheep&#39;s blog">
<meta property="og:description" content="一开始是看的Stable diffusion的,以为是这里开始讲conditioning mechanisms的,后来发现他们idea是从LDM过来的,那么这次就学习一下LDM,这次没来得及看模型,这两天稍微有点忙,看周末会不会有空补一下了. Latent Diffusion Models LDM区别于我们之前看的一般diffusion model主要在于两点  之前我们对一般扩散模型是直接喂图片">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/04/11/diffusion-model-2-LDM/ldm_cross_attention.png">
<meta property="og:image" content="http://example.com/2024/04/11/diffusion-model-2-LDM/cross_attention.png">
<meta property="article:published_time" content="2024-04-11T20:46:09.000Z">
<meta property="article:modified_time" content="2024-04-20T19:03:19.849Z">
<meta property="article:author" content="blacsheep">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/04/11/diffusion-model-2-LDM/ldm_cross_attention.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>diffusion model(2)-LDM</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/04/20/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0LDM-1/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/04/10/diffusion-model-1/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/04/11/diffusion-model-2-LDM/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&text=diffusion model(2)-LDM"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&is_video=false&description=diffusion model(2)-LDM"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=diffusion model(2)-LDM&body=Check out this article: http://example.com/2024/04/11/diffusion-model-2-LDM/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&name=diffusion model(2)-LDM&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/04/11/diffusion-model-2-LDM/&t=diffusion model(2)-LDM"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#latent-diffusion-models"><span class="toc-number">1.</span> <span class="toc-text">Latent Diffusion Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#perceptual-image-compression"><span class="toc-number">1.1.</span> <span class="toc-text">Perceptual Image Compression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%9B%A0%E5%AD%90-f"><span class="toc-number">1.2.</span> <span class="toc-text">采样因子 $ f $</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-attention"><span class="toc-number">1.3.</span> <span class="toc-text">cross attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation4.13"><span class="toc-number">2.</span> <span class="toc-text">implementation(4.13)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#perceptual-image-compression%E7%9B%B8%E5%85%B3"><span class="toc-number">2.1.</span> <span class="toc-text">Perceptual Image Compression相关</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#vqvector-quantization"><span class="toc-number">2.1.1.</span> <span class="toc-text">VQ(Vector Quantization)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#resnet"><span class="toc-number">2.1.2.</span> <span class="toc-text">resnet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#encoder"><span class="toc-number">2.1.3.</span> <span class="toc-text">encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#decoder"><span class="toc-number">2.1.4.</span> <span class="toc-text">decoder</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#text-processing"><span class="toc-number">2.2.</span> <span class="toc-text">text processing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">2.3.</span> <span class="toc-text">attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation4.14-4.15"><span class="toc-number">3.</span> <span class="toc-text">implementation(4.14-4.15)</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        diffusion model(2)-LDM
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">blacsheep</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-04-11T20:46:09.000Z" class="dt-published" itemprop="datePublished">2024-04-11</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/ML/">ML</a>
    </div>


      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>一开始是看的Stable diffusion的,以为是这里开始讲conditioning mechanisms的,后来发现他们idea是从LDM过来的,那么这次就学习一下LDM,这次没来得及看模型,这两天稍微有点忙,看周末会不会有空补一下了.</p>
<h2 id="latent-diffusion-models">Latent Diffusion Models</h2>
<p>LDM区别于我们之前看的一般diffusion model主要在于两点</p>
<ol type="1">
<li><p>之前我们对一般扩散模型是直接喂图片数据. 而LDM对其进行了优化, 它设计了一个encoder和一个decoder, encoder对输入图片进行压缩将其转化到latent space, 然后进行一般diffusion操作之后我们同样会生成一个latent vector, 最后我们对这个生成的latent vector进行decode让其恢复到像素空间即可完成图片生成. 本质高维变低维进行处理. 论文将此方法称为Perceptual Compression.</p></li>
<li><p>第二个区别就是条件生成了, 其通过在unet主干网络添加cross-attention来进行针对prompt的图片生成.</p></li>
</ol>
<h3 id="perceptual-image-compression">Perceptual Image Compression</h3>
<p>本质实现一个编码解码器,然后这个编码解码器先去学习将图片映射到latent space,第二部再去训练diffusion model作为内层.</p>
<blockquote>
<p>感知压缩本质上是一个tradeoff，之前的很多扩散模型没有使用这个技巧也可以进行，但原有的非感知压缩的扩散模型有一个很大的问题在于，由于在像素空间上训练模型，如果我们希望生成一张分辨率很高的图片，这就意味着我们训练的空间也是一个很高维的空间。引入感知压缩就是说通过VAE这类自编码模型对原图片进行处理，忽略掉图片中的高频信息，只保留重要、基础的一些特征。这种方法带来的的好处就像引文部分说的一样，能够<strong>大幅降低训练和采样阶段的计算复杂度</strong>，让文图生成等任务能够在消费级GPU上，在10秒级别时间生成图片，大大降低了落地门槛。</p>
</blockquote>
<h3 id="采样因子-f">采样因子 $ f $</h3>
<p>f = H / h, 其中如果f = 1就相当于没有对空间进行压缩, 如果f越大, 就说明压缩程度越高,需要占用的资源更少但同样也更容易失真. 论文对比了f在{1,2,4,8,16,32},发现在4-16效果较好且推荐LDM-4和LDM-8</p>
<h3 id="cross-attention">cross attention</h3>
<p>这个就是为什么LDM可以做到text_to_img,针对text而言可以先做一些tokenization和embedding,之后和传入的img_feature一起做cross attention.</p>
<p>而作者则是写到了为了预处理y,他们引入了一种新的domain specific encoder $ $ 从而把任意源y编码到中间态 $ (y) $</p>
<p><img src="ldm_cross_attention.png" /></p>
<p>然后就是Q为img_feature,KV为text_embedding的attention机制.</p>
<p><img src="cross_attention.png" /></p>
<h2 id="implementation4.13">implementation(4.13)</h2>
<p>首先代码是参考的CompVis的<a target="_blank" rel="noopener" href="https://github.com/CompVis/latent-diffusion">repo</a>,首先简要分析一下各个部分的实现,然后看下这周末有没有可能基于之前的简易diffusion model来实现一个简易的ldm.</p>
<p>既然之前已经分析过了一般diffusion model的工作原理,那么这里就先看看几个新增的部分如何实现, 具体就是encoder,decoder, text_processing, attention mechanism.</p>
<h3 id="perceptual-image-compression相关">Perceptual Image Compression相关</h3>
<p>主体在ldm.models.autoencoder.VQModelInterface,其中关键部分为处理encode的decode的部分. 另外中间还有一个VQ过程.</p>
<h4 id="vqvector-quantization">VQ(Vector Quantization)</h4>
<p><del>似乎是一个很早的东西,不过我个人第一次见到,记录一下. 参考文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/FRkid/article/details/40654841">VQ</a></del> <del>说白了就是VQ会将某个区间内的东西用某个值替换,比如0-1全部映射到0,1-2全部映射到1.</del> <del>不过缺陷就是完全空间划分可能导致不均匀,比如我们需要将0-255映射到4bit,然后所有值都在0-15,那么最后得到的图像就会全部为0.</del> <del>优化方式为聚类算法,比如KMean,手动选择所有点中的centroids,然后再针对centroids进行映射.</del></p>
<p>4.16: 有点理解错了. 这里的VQ来自于CompVis的另一篇论文,<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841">Taming Transformers for High-Resolution Image Synthesis</a>,本质其实是VQGAN,所以大概的历史发展大概是VQVAE-&gt;VQGAN-&gt;LDM,其中LDM也可以看作是VQ+DDPM. 这个VQ就是压缩图片到隐藏空间的核心所在, 也是所谓的First stage model.</p>
<p>最本质的VQ其实还是上面划掉的部分,化连续为离散,贴一篇讲解<a target="_blank" rel="noopener" href="https://www.spaces.ac.cn/archives/6760#VQ-VAE">VQVAE</a></p>
<p>从这篇文章看到VQVAE一开始用VQ解决的问题是 1. PixelCNN这种需要离散数据的模型应该如何进行降维,建模,恢复. 2. 对待离散数据中无梯度部分应该如何处理(Straight-Through Estimator).</p>
<p>而VQGAN, LDM继续延用了VQ这一思想, 这些模型把VQ丢到了第一阶段, 目的是训练一个所谓的autoencoder.</p>
<h4 id="resnet">resnet</h4>
<p>首先encoder和decoder里面每一个维度变化都是resnet作为基础块来处理的,对于他们写的resnet不需要太过关注内部处理,只用看最后返回是h+x即可. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L121</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, temb</span>):</span><br><span class="line">        h = x</span><br><span class="line">        h = self.norm1(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = self.conv1(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            h = h + self.temb_proj(nonlinearity(temb))[:,:,<span class="literal">None</span>,<span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        h = self.norm2(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = self.dropout(h)</span><br><span class="line">        h = self.conv2(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.in_channels != self.out_channels:</span><br><span class="line">            <span class="keyword">if</span> self.use_conv_shortcut:</span><br><span class="line">                x = self.conv_shortcut(x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = self.nin_shortcut(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x+h</span><br></pre></td></tr></table></figure></p>
<h4 id="encoder">encoder</h4>
<p><a target="_blank" rel="noopener" href="https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368">代码部分</a></p>
<p>首先构造函数部分就可以看到一个核为3的卷积层conv_in, 然后依据设置的残差块的个数初始化残差块,然后middle设置俩残差块,中间夹着一个attention块,然后forward的时候就传参即可.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># timestep embedding</span></span><br><span class="line">        temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># downsampling</span></span><br><span class="line">        hs = [self.conv_in(x)]</span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(self.num_resolutions):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks):</span><br><span class="line">                h = self.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(self.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = self.down[i_level].attn[i_block](h)</span><br><span class="line">                hs.append(h)</span><br><span class="line">            <span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">                hs.append(self.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># middle</span></span><br><span class="line">        h = hs[-<span class="number">1</span>]</span><br><span class="line">        h = self.mid.block_1(h, temb)</span><br><span class="line">        h = self.mid.attn_1(h)</span><br><span class="line">        h = self.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">        h = self.norm_out(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = self.conv_out(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>
<h4 id="decoder">decoder</h4>
<p>大致和encoder相似,不过区别encoder是从高维到低维,这里decoder中的i_level是从reverse里面取,即低维度到高维度 然后除了非线性层的处理,其他都是encoder的逆序处理.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="comment">#assert z.shape[1:] == self.z_shape[1:]</span></span><br><span class="line">        self.last_z_shape = z.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># timestep embedding</span></span><br><span class="line">        temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># z to block_in</span></span><br><span class="line">        h = self.conv_in(z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># middle</span></span><br><span class="line">        h = self.mid.block_1(h, temb)</span><br><span class="line">        h = self.mid.attn_1(h)</span><br><span class="line">        h = self.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># upsampling</span></span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(self.num_resolutions)):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks+<span class="number">1</span>):</span><br><span class="line">                h = self.up[i_level].block[i_block](h, temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(self.up[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = self.up[i_level].attn[i_block](h)</span><br><span class="line">            <span class="keyword">if</span> i_level != <span class="number">0</span>:</span><br><span class="line">                h = self.up[i_level].upsample(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">        <span class="keyword">if</span> self.give_pre_end:</span><br><span class="line">            <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line">        h = self.norm_out(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = self.conv_out(h)</span><br><span class="line">        <span class="keyword">if</span> self.tanh_out:</span><br><span class="line">            h = torch.tanh(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>
<h3 id="text-processing">text processing</h3>
<p>这部分主要是ldm.modules.encoders.modules.BERTEmbedder,这里默认使用huggingface的BertTokenizerFast.from_pretrained('bert-base-uncased').</p>
<p>然后TransformWrapper就是NLP的token_emb,position_emb,attention三件套. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Uses the BERT tokenizr model and add some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size=<span class="number">30522</span>, max_seq_len=<span class="number">77</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="string">&quot;cuda&quot;</span>,use_tokenizer=<span class="literal">True</span>, embedding_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.use_tknz_fn = use_tokenizer</span><br><span class="line">        <span class="keyword">if</span> self.use_tknz_fn:</span><br><span class="line">            self.tknz_fn = BERTTokenizer(vq_interface=<span class="literal">False</span>, max_length=max_seq_len)</span><br><span class="line">        self.device = device</span><br><span class="line">        self.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer),</span><br><span class="line">                                              emb_dropout=embedding_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">if</span> self.use_tknz_fn:</span><br><span class="line">            tokens = self.tknz_fn(text)<span class="comment">#.to(self.device)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens = text</span><br><span class="line">        z = self.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># output of length 77</span></span><br><span class="line">        <span class="keyword">return</span> self(text)</span><br></pre></td></tr></table></figure></p>
<h3 id="attention">attention</h3>
<p>不得不说这个代码是真的抽象,这个condition真的传了一万个函数一万个类, 不过本质还是在Unet里面. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/attention.py#L152</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, context=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        h = self.heads</span><br><span class="line"></span><br><span class="line">        q = self.to_q(x)</span><br><span class="line">        context = default(context, x)</span><br><span class="line">        k = self.to_k(context)</span><br><span class="line">        v = self.to_v(context)</span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; (b h) n d&#x27;</span>, h=h), (q, k, v))</span><br><span class="line"></span><br><span class="line">        sim = einsum(<span class="string">&#x27;b i d, b j d -&gt; b i j&#x27;</span>, q, k) * self.scale</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exists(mask):</span><br><span class="line">            mask = rearrange(mask, <span class="string">&#x27;b ... -&gt; b (...)&#x27;</span>)</span><br><span class="line">            max_neg_value = -torch.finfo(sim.dtype).<span class="built_in">max</span></span><br><span class="line">            mask = repeat(mask, <span class="string">&#x27;b j -&gt; (b h) () j&#x27;</span>, h=h)</span><br><span class="line">            sim.masked_fill_(~mask, max_neg_value)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention, what we cannot get enough of</span></span><br><span class="line">        attn = sim.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        out = einsum(<span class="string">&#x27;b i j, b j d -&gt; b i d&#x27;</span>, attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;(b h) n d -&gt; b n (h d)&#x27;</span>, h=h)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure></p>
<p>那其实从结果来看就是x和condition进行cross attention然后输出丢进Unet,然后encode和decode那一套. 从代码执行逻辑来看和之前分析的确实也差不多,今天的分析暂时先到这,下次继续看,然后试试能不能自己来写一个简易LDM.</p>
<h2 id="implementation4.14-4.15">implementation(4.14-4.15)</h2>
<p>看了下dataset和具体实现部分,官方的代码感觉有点难读,然后注意到encoder的部分似乎实现和之前想的有点差异,第一阶段的encoder和decoder其实只是first_stage的其中一部分,最终使用似乎还是<a target="_blank" rel="noopener" href="https://github.com/CompVis/latent-diffusion/blob/main/ldm/models/autoencoder.py">ldm.models.autoencoder.VQModelInterface</a>. 然后模型的训练似乎也和之前想的不太一样,看官方的意思似乎是得先<strong>训练好</strong>autoencoder,之后再直接对autoencoder进行使用:encoder进latent space然后计算,预测之后decode做返回. 不过这里并不太了解encoder训练相关,最近还得多看一下,能找到的资料也非常少,官方的github代码可读性也不是很行.</p>
<p>然后dataset弄的MSCOCO,练手小项目,随缘使用一个600k就行了,不过目前只是大概确认了一下数据,最后大概还需要写个dataset和dataloader之类的载入类.</p>
<blockquote>
<p>https://arxiv.org/abs/2012.09841 https://arxiv.org/abs/2112.10752 https://www.spaces.ac.cn/archives/6760#VQ-VAE https://zhuanlan.zhihu.com/p/582693939 https://www.cnblogs.com/zackstang/p/17324257.html#_label5</p>
</blockquote>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#latent-diffusion-models"><span class="toc-number">1.</span> <span class="toc-text">Latent Diffusion Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#perceptual-image-compression"><span class="toc-number">1.1.</span> <span class="toc-text">Perceptual Image Compression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%9B%A0%E5%AD%90-f"><span class="toc-number">1.2.</span> <span class="toc-text">采样因子 $ f $</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-attention"><span class="toc-number">1.3.</span> <span class="toc-text">cross attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation4.13"><span class="toc-number">2.</span> <span class="toc-text">implementation(4.13)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#perceptual-image-compression%E7%9B%B8%E5%85%B3"><span class="toc-number">2.1.</span> <span class="toc-text">Perceptual Image Compression相关</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#vqvector-quantization"><span class="toc-number">2.1.1.</span> <span class="toc-text">VQ(Vector Quantization)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#resnet"><span class="toc-number">2.1.2.</span> <span class="toc-text">resnet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#encoder"><span class="toc-number">2.1.3.</span> <span class="toc-text">encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#decoder"><span class="toc-number">2.1.4.</span> <span class="toc-text">decoder</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#text-processing"><span class="toc-number">2.2.</span> <span class="toc-text">text processing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">2.3.</span> <span class="toc-text">attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation4.14-4.15"><span class="toc-number">3.</span> <span class="toc-text">implementation(4.14-4.15)</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/04/11/diffusion-model-2-LDM/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&text=diffusion model(2)-LDM"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&is_video=false&description=diffusion model(2)-LDM"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=diffusion model(2)-LDM&body=Check out this article: http://example.com/2024/04/11/diffusion-model-2-LDM/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&title=diffusion model(2)-LDM"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/04/11/diffusion-model-2-LDM/&name=diffusion model(2)-LDM&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/04/11/diffusion-model-2-LDM/&t=diffusion model(2)-LDM"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    blacsheep
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
