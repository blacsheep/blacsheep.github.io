<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="今天本来想补一下NLP尤其是GPT的相关知识,但看到一篇很好的博客讲了NLP领域的发展,这里记录一下. 关于ChatGPT的思考. 写的很长,但是讲清楚了自然语言处理这个领域的每个阶段,能够学习到不少东西. DeepLearning word embedding经历 最开始NLP不同于其他学习类型,NLP的文本类型是离散类型而不是像数字类型一样的连续类型. 处理起来第一步肯定是类型转化,最简单的转">
<meta property="og:type" content="article">
<meta property="og:title" content="从RNN到GPT(nlp背景学习)">
<meta property="og:url" content="http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="blacsheep&#39;s blog">
<meta property="og:description" content="今天本来想补一下NLP尤其是GPT的相关知识,但看到一篇很好的博客讲了NLP领域的发展,这里记录一下. 关于ChatGPT的思考. 写的很长,但是讲清楚了自然语言处理这个领域的每个阶段,能够学习到不少东西. DeepLearning word embedding经历 最开始NLP不同于其他学习类型,NLP的文本类型是离散类型而不是像数字类型一样的连续类型. 处理起来第一步肯定是类型转化,最简单的转">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/animal_street.png">
<meta property="article:published_time" content="2024-04-22T21:31:32.000Z">
<meta property="article:modified_time" content="2024-04-24T17:01:44.275Z">
<meta property="article:author" content="blacsheep">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/animal_street.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>从RNN到GPT(nlp背景学习)</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/04/26/Llama%E5%AD%A6%E4%B9%A0%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/04/22/Diffusers%E5%AD%A6%E4%B9%A0-2-LoRA/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&text=从RNN到GPT(nlp背景学习)"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&is_video=false&description=从RNN到GPT(nlp背景学习)"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=从RNN到GPT(nlp背景学习)&body=Check out this article: http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&name=从RNN到GPT(nlp背景学习)&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&t=从RNN到GPT(nlp背景学习)"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#deeplearning"><span class="toc-number">1.</span> <span class="toc-text">DeepLearning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#word-embedding%E7%BB%8F%E5%8E%86"><span class="toc-number">1.1.</span> <span class="toc-text">word embedding经历</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#latent-semantic-analysis"><span class="toc-number">1.1.1.</span> <span class="toc-text">Latent Semantic Analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#word2vec"><span class="toc-number">1.1.2.</span> <span class="toc-text">word2vec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#glove"><span class="toc-number">1.1.3.</span> <span class="toc-text">GloVe</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dnn-cnn-rnn-lstm"><span class="toc-number">1.2.</span> <span class="toc-text">DNN-&gt;CNN-&gt;RNN-&gt;LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#seq2seq-attention"><span class="toc-number">1.3.</span> <span class="toc-text">Seq2Seq &amp; Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%91%E4%BB%A3deep-learning"><span class="toc-number">2.</span> <span class="toc-text">近代Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer"><span class="toc-number">2.1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention"><span class="toc-number">2.1.1.</span> <span class="toc-text">Self-Attention</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%92%E6%9B%B2"><span class="toc-number">2.2.</span> <span class="toc-text">插曲</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#elmo"><span class="toc-number">2.3.</span> <span class="toc-text">ELMo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt"><span class="toc-number">2.4.</span> <span class="toc-text">GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pretraining"><span class="toc-number">2.4.1.</span> <span class="toc-text">Pretraining</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E7%9A%84fine-tuning"><span class="toc-number">2.4.2.</span> <span class="toc-text">监督的Fine-Tuning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert"><span class="toc-number">2.5.</span> <span class="toc-text">BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt2-%E5%8F%8A%E4%BB%A5%E5%90%8E"><span class="toc-number">2.6.</span> <span class="toc-text">GPT2 及以后</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        从RNN到GPT(nlp背景学习)
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">blacsheep</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-04-22T21:31:32.000Z" class="dt-published" itemprop="datePublished">2024-04-23</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/ML-NLP/">ML NLP</a>
    </div>


      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>今天本来想补一下NLP尤其是GPT的相关知识,但看到一篇很好的博客讲了NLP领域的发展,这里记录一下.</p>
<p><a target="_blank" rel="noopener" href="https://fancyerii.github.io/2023/02/20/about-chatgpt/">关于ChatGPT的思考</a>.</p>
<p>写的很长,但是讲清楚了自然语言处理这个领域的每个阶段,能够学习到不少东西.</p>
<h2 id="deeplearning">DeepLearning</h2>
<h3 id="word-embedding经历">word embedding经历</h3>
<p>最开始NLP不同于其他学习类型,NLP的文本类型是离散类型而不是像数字类型一样的连续类型. 处理起来第一步肯定是类型转化,最简单的转化即one-hot. 每个标签算一列,n个标签n个列.</p>
<p>这就导致one-hot有致命问题,文本如果涉及成千上万的单词,那我们几千几万列的稀疏矩阵,处理起来就很麻烦.</p>
<p>其次,同种物品或者完全不同的物品在某种程度应该有相同点或区分点,而在one-hot里面就只有0或者1. 比如猫和狗,在one-hot的层面就是完全不同的东西,而不会考虑都是动物类型.</p>
<p>所以一种更理想的形式,以稠密向量来表示某个词的意义就成为了方向. 由此衍生出了一些处理方式.</p>
<h4 id="latent-semantic-analysis"><u>Latent Semantic Analysis</u></h4>
<p>包括occurence matrix和SVD.</p>
<p>occurence就是行表示词,列表示文档,每个元素表示词和文档的频率关系,有tf,idf,tf-idf等一类.</p>
<p>SVD即矩阵分解,将矩阵分解为左奇异向量右奇异向量和包含奇异值的对角矩阵,从对角矩阵里面选取k个奇异值最大的元素做相乘从而达到从n维降维到k维的目的.</p>
<h4 id="word2vec"><u>word2vec</u></h4>
<p>包含CBOW和Skip-Gram</p>
<p>CBOW即从context预测center word, Skip-Gram从center word预测context.</p>
<h4 id="glove"><u>GloVe</u></h4>
<p>Co-occurence + context window + decreasing weight</p>
<p>Co-occurence从词库构建矩阵, context window做规定大小内统计, decreasing weight负责在context window中距离不同的权重衰减.</p>
<h3 id="dnn-cnn-rnn-lstm">DNN-&gt;CNN-&gt;RNN-&gt;LSTM</h3>
<p>有了word embedding,就可以直接丢上nn进行最初步的训练了.</p>
<p><strong><u>NN+CNN</u></strong>: 一开始是全连层, 后来CV领域的CNN也进入使用(可以看作是利用卷积获取某种程度的context window从而获取上下文).</p>
<p><strong><u>RNN</u></strong>: 再之后是RNN, RNN更加优秀的点在于它每一个输出都是基于前面所有的输出, 后续序列的预测保留了前面的"记忆", 所以理论来讲RNN可以训练任意长度序列.</p>
<p><strong><u>LSTM, GRU</u></strong>: 再之后是LSTM和GRU, 解决的是RNN中梯度消失的问题. 比如LSTM就是通过门控对RNN加上短期记忆的支持和长期记忆的选择遗忘从而解决RNN长度过长的时候会遗忘过去信息的问题, 而GRU是门控简化的LSTM.</p>
<h3 id="seq2seq-attention">Seq2Seq &amp; Attention</h3>
<p><strong>Seq2Seq</strong>解决的问题是<strong>文本翻译</strong>或者<strong>从文本生成文本</strong>的问题, 具体做法就是input丢到encoder将输入编码成一个latent vector然后再由decoder对lantent vector进行还原.</p>
<p><strong>Attention</strong>: 然而把整个长句全部编码到一个latent vector是困难的, 所以这里引入了attention. 即decoder对latent vector进行解码的时候会受到encoder中所有步骤的信息-即注意力权重的影响, 从而decoder知道哪个步骤我需要着重关注而哪些词无关紧要.</p>
<ul>
<li>在Encoder-Decoder框架下，Decoder在解码的时候会“注意”到Encoder的隐状态序列，从而利用这些信息更好的解码。</li>
<li>Encoder会把每一个输入的Token(可以理解为词)都编码成一个向量，在Decoder进行t时刻计算的时候，除了t-1时刻的隐状态和当前时刻的输入，注意力机制还可以参考Encoder所有时刻的输入。比如上面输入是"欢迎 来 北京"，那么在翻译第一个词的注意力可能是这样: (0.7,0.2,0.1)，表示当前最关注的是输入的第一个词，但是也会参考其它的词。假设Encoder的3个输出分别是𝑦1,𝑦2,𝑦3，那么我们可以用attention概率加权得到当前时刻的context向量0.7*𝑦1 + 0.2*𝑦2 + 0.1*𝑦3.</li>
</ul>
<h2 id="近代deep-learning">近代Deep Learning</h2>
<h3 id="transformer">Transformer</h3>
<p>LSTM搭配Attention来处理Seq2Seq相关问题流行了起来, 但是RNN底层逻辑还是有问题:</p>
<ol type="1">
<li>RNN下一步建立在上一步计算上,所以无法并行</li>
<li>当前RNN仅单向, 如果文本里面的context在当前时间t之后, 那么模型就无法获得信息.</li>
</ol>
<p><img src="animal_street.png" /></p>
<h4 id="self-attention">Self-Attention</h4>
<p>Transformer也是来自著名论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, 该论文提出了使用SelfAttention来取代传统LSTM和GRU.</p>
<p>在Encoder-Decoder框架下，Decoder在解码的时候会“注意”到Encoder的隐状态序列，从而利用这些信息更好的解码.</p>
<p>在Transformer中, SelfAttention利用整个序列的embedding来计算当前token的embedding, 即每一个新的embedding都会包含有整个序列的信息.</p>
<p><span class="math display">\[ x_i’=\sum_{j=1}^{n}w_{ji}x_j \]</span></p>
<h3 id="插曲">插曲</h3>
<p><a target="_blank" rel="noopener" href="https://fancyerii.github.io/2023/02/20/about-chatgpt/#%E5%BD%93%E6%97%B6%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">当时存在的问题</a></p>
<p>作者这里指出当时自然语言处理领域缺少单任务训练数据,导致解决一个问题可能还会连带解决很多中间问题. 作者表示这是技术发展不够的一种表现, 并且正是由于我们没有对专一任务的数据集, 最好的解决方案是能够让模型从无标注的文本中学习.</p>
<p>确实是正确的思考方式,不过当时我可能连numpy都没用过,就只能体会一下当时的背景了.</p>
<h3 id="elmo">ELMo</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>, 给人直观感觉就是解决word vector层面的问题. 其能够基于上下文来从无标注的文本中学习相关Embedding(解决了前面提到的数据稀少问题), 并且能够动态生成词向量供模型使用.</p>
<p>以前的word embedding主要为了解决两个问题.</p>
<ol type="1">
<li>语法语义, 某词表示什么意思, 有什么特点.</li>
<li>不同场合的同词使用.</li>
</ol>
<p>word2vec和GloVe解决了第一个问题,能够用向量表示某些词的意思,但是依然存在同词不同场景的使用情景. 经典的bank可以指银行也可以是水坝. 但是它们针对同一个确定token的word vector却总是相同的.</p>
<p>ELMo做出了两方面贡献</p>
<ol type="1">
<li>双向训练, 从前往后训练以及从后往前训练, 从而此token可以获得整个全文的信息而不至于从前往后的时候总是未知后续信息.</li>
<li>动态embedding</li>
</ol>
<p>第一点容易理解, 在base model为LSTM的情况下, 单向训练确实会缺少大于t时间的信息, 双向训练获取全文信息也就容易理解.</p>
<p>第二点的动态embedding实现就比较巧妙了, token本身从GloVe或者word2vec就生成了自己的embedding, 那我们在针对某个词之前就用一个全连层连接原始的embedding和新产生的context, 从而整合context信息和原始词义达到动态embedding的效果.</p>
<p>当然ELMo只是一个上游任务, 后续具体任务可以直接搭在ELMo的基础之上.</p>
<h3 id="gpt">GPT</h3>
<p>Generative Pre-trained Transformer. 其思想相当于是使用transformer的decoder直接进行文本的生成, 最后依据具体任务对transformer进行finetuning.</p>
<h4 id="pretraining">Pretraining</h4>
<p>Pretraining即训练模型使用transformer的decoder直接进行文本的生成.</p>
<p>但是这里存在问题, transformer的decoder里面用到了全局self attention, 其包含了前后文所有信息, 如果我们预先知道了全文信息那预测就是相当于作弊, 因此在训练的时候我们会对后续输入进行mask, 即下一字符的生成仅仅和前面的字符相关.</p>
<p>即使用的是Masked Self Attention.</p>
<h4 id="监督的fine-tuning">监督的Fine-Tuning</h4>
<p>无监督的Pretraining之后，我们再去针对特定任务进行Fine-Tuning.</p>
<h3 id="bert">BERT</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> 全称Bidirectional Encoder Representations from Transformers, 即双向的transformer encoder模型.</p>
<p>在transformer的基础之上, 即往前看也往后看, 整合整个句子的context information.</p>
<p>然后就是和GPT同样的问题, 如果我们已经具备了全文知识, 那么我们的训练其实是在作弊. 同样BERT也使用了mask.</p>
<p>举个例子, 一个句子里面我们mask掉15%的token, 那么一旦一个token落在这15%中, 我们可以选择替换成&lt;mask&gt;, 或者替换成其他token,又或者保持原有token, 然后让模型去训练. 此过程即训练模型并不能太过依赖词本身而是要考虑上下文.</p>
<p>除此之外,BERT还会对序列做Segment的区分, 从而判断两个句子是否是相连的, 甚至可以后续用来做问答使用.</p>
<h3 id="gpt2-及以后">GPT2 及以后</h3>
<p>这部分内容挺多的,近期可以把原论文读一读然后再回头来看,毕竟不看论文总感觉少点意思.</p>
<p>不过NLP的发展之路也挺有意思的, 在此记录一下, 感谢<a target="_blank" rel="noopener" href="https://fancyerii.github.io/">李理</a>的精彩记录, 受益匪浅.</p>
<blockquote>
<p>https://fancyerii.github.io/2023/02/20/about-chatgpt<br />
https://blog.csdn.net/qq_43689179/article/details/98478617<br />
https://blog.csdn.net/qq_45848817/article/details/136889730<br />
https://www.fanyeong.com/2018/02/19/glove-in-detail/<br />
https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/篇章2-Transformer相关原理/2.4-图解GPT.md</p>
</blockquote>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#deeplearning"><span class="toc-number">1.</span> <span class="toc-text">DeepLearning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#word-embedding%E7%BB%8F%E5%8E%86"><span class="toc-number">1.1.</span> <span class="toc-text">word embedding经历</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#latent-semantic-analysis"><span class="toc-number">1.1.1.</span> <span class="toc-text">Latent Semantic Analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#word2vec"><span class="toc-number">1.1.2.</span> <span class="toc-text">word2vec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#glove"><span class="toc-number">1.1.3.</span> <span class="toc-text">GloVe</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dnn-cnn-rnn-lstm"><span class="toc-number">1.2.</span> <span class="toc-text">DNN-&gt;CNN-&gt;RNN-&gt;LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#seq2seq-attention"><span class="toc-number">1.3.</span> <span class="toc-text">Seq2Seq &amp; Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%91%E4%BB%A3deep-learning"><span class="toc-number">2.</span> <span class="toc-text">近代Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer"><span class="toc-number">2.1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention"><span class="toc-number">2.1.1.</span> <span class="toc-text">Self-Attention</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%92%E6%9B%B2"><span class="toc-number">2.2.</span> <span class="toc-text">插曲</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#elmo"><span class="toc-number">2.3.</span> <span class="toc-text">ELMo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt"><span class="toc-number">2.4.</span> <span class="toc-text">GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pretraining"><span class="toc-number">2.4.1.</span> <span class="toc-text">Pretraining</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E7%9A%84fine-tuning"><span class="toc-number">2.4.2.</span> <span class="toc-text">监督的Fine-Tuning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert"><span class="toc-number">2.5.</span> <span class="toc-text">BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt2-%E5%8F%8A%E4%BB%A5%E5%90%8E"><span class="toc-number">2.6.</span> <span class="toc-text">GPT2 及以后</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&text=从RNN到GPT(nlp背景学习)"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&is_video=false&description=从RNN到GPT(nlp背景学习)"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=从RNN到GPT(nlp背景学习)&body=Check out this article: http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&title=从RNN到GPT(nlp背景学习)"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&name=从RNN到GPT(nlp背景学习)&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/04/23/%E4%BB%8ERNN%E5%88%B0GPT-nlp%E8%83%8C%E6%99%AF%E5%AD%A6%E4%B9%A0/&t=从RNN到GPT(nlp背景学习)"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    blacsheep
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
