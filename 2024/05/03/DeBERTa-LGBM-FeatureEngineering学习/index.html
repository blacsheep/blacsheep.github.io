<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="参考的notebook的地址: DeBERTa &amp; LightGBM for Automated Essay Scoring. 首先会简要学习一下deberta,然后会看下notebook里面是如何做feature engineering的. DeBERTa v1: DeBERTa: Decoding-enhanced BERT with Disentangled Attention v3">
<meta property="og:type" content="article">
<meta property="og:title" content="DeBERTa+LGBM+FeatureEngineering学习">
<meta property="og:url" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="blacsheep&#39;s blog">
<meta property="og:description" content="参考的notebook的地址: DeBERTa &amp; LightGBM for Automated Essay Scoring. 首先会简要学习一下deberta,然后会看下notebook里面是如何做feature engineering的. DeBERTa v1: DeBERTa: Decoding-enhanced BERT with Disentangled Attention v3">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/bert_embedding.png">
<meta property="og:image" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/disentangled_attention.png">
<meta property="og:image" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/cross_relationship.png">
<meta property="og:image" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/deberta_res.png">
<meta property="og:image" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/tf-idf.png">
<meta property="article:published_time" content="2024-05-03T21:28:49.000Z">
<meta property="article:modified_time" content="2024-05-22T11:30:08.064Z">
<meta property="article:author" content="blacsheep">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/bert_embedding.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>DeBERTa+LGBM+FeatureEngineering学习</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/05/08/Norm%E7%9B%B8%E5%85%B3/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/05/03/SuperGLUE%E4%BB%A5%E5%8F%8A%E5%90%8E%E7%BB%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%90%91/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&text=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&is_video=false&description=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=DeBERTa+LGBM+FeatureEngineering学习&body=Check out this article: http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&name=DeBERTa+LGBM+FeatureEngineering学习&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&t=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#deberta"><span class="toc-number">1.</span> <span class="toc-text">DeBERTa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#disentangled-attention"><span class="toc-number">1.1.</span> <span class="toc-text">Disentangled Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#enhanced-mask-decoder"><span class="toc-number">1.2.</span> <span class="toc-text">Enhanced Mask Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scale-invariant-fine-tuningsift"><span class="toc-number">1.3.</span> <span class="toc-text">SCALE INVARIANT FINE-TUNING(SIFT)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#deberta-feature-engineering-lgbm"><span class="toc-number">2.</span> <span class="toc-text">DeBERTa + Feature Engineering + LGBM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#import"><span class="toc-number">2.1.</span> <span class="toc-text">import</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#baseline-deberta"><span class="toc-number">2.2.</span> <span class="toc-text">baseline DeBERTa</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-engineering"><span class="toc-number">2.3.</span> <span class="toc-text">Feature Engineering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#text---paragraph"><span class="toc-number">2.3.1.</span> <span class="toc-text">1. text -&gt; paragraph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#data-preprocessing"><span class="toc-number">2.3.2.</span> <span class="toc-text">2. data preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count-error"><span class="toc-number">2.3.3.</span> <span class="toc-text">3. count error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%B3%A2%E5%A4%84%E7%90%86paragraph"><span class="toc-number">2.3.4.</span> <span class="toc-text">第一波处理(paragraph)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%B3%A2%E5%A4%84%E7%90%86sentence"><span class="toc-number">2.3.5.</span> <span class="toc-text">第二波处理(sentence)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#word"><span class="toc-number">2.3.6.</span> <span class="toc-text">word</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tfidf"><span class="toc-number">2.3.7.</span> <span class="toc-text">tfidf</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count"><span class="toc-number">2.3.8.</span> <span class="toc-text">count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88deberta"><span class="toc-number">2.3.9.</span> <span class="toc-text">整合deberta</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#evaluation-metrics"><span class="toc-number">2.3.10.</span> <span class="toc-text">evaluation metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E6%95%B0%E6%8D%AE%E4%B8%A2lgbm"><span class="toc-number">2.3.11.</span> <span class="toc-text">整合数据,丢LGBM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#test-data-prediction"><span class="toc-number">2.3.12.</span> <span class="toc-text">test data prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#update2024.05.22"><span class="toc-number">2.3.13.</span> <span class="toc-text">update(2024.05.22)</span></a></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        DeBERTa+LGBM+FeatureEngineering学习
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">blacsheep</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-05-03T21:28:49.000Z" class="dt-published" itemprop="datePublished">2024-05-03</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/ML-NLP/">ML NLP</a>
    </div>


      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>参考的notebook的地址: <a target="_blank" rel="noopener" href="https://www.kaggle.com/code/qamarmath/deberta-lightgbm-for-automated-essay-scoring">DeBERTa &amp; LightGBM for Automated Essay Scoring</a>.</p>
<p>首先会简要学习一下deberta,然后会看下notebook里面是如何做feature engineering的.</p>
<h1 id="deberta">DeBERTa</h1>
<p>v1: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.03654">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a></p>
<p>v3: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.09543">DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</a></p>
<p>其中v1主要对两个部分做了改进: 一个是Disentangled Attention, 另一个是Enhanced mask decoder, 然后文中还提及到了新的对抗训练改进.</p>
<h2 id="disentangled-attention">Disentangled Attention</h2>
<p>我们知道bert里面对embedding的处理是相加的方法</p>
<p><img src="bert_embedding.png" /></p>
<p>不同于bert一个词只有一个content embedding和一个position embedding, 求和即位词嵌入; DeBERTa中每个词都有两个vector表示,而position embedding也是relative embedding,然后token_i和token_j的cross attention就可以由四个部分组成: content -&gt; content, content -&gt; positon, position -&gt; content, position -&gt; position.</p>
<p><img src="disentangled_attention.png" /></p>
<p>然后后面具体的计算还挺复杂的, 不过说白了其实还是矩阵交叉, 有点类似之前看的ffm.</p>
<p><img src="cross_relationship.png" /></p>
<h2 id="enhanced-mask-decoder">Enhanced Mask Decoder</h2>
<p>前面讲到DeBERTa的attention用的是relative position,对比bert里面是input的时候直接加起来了,所以到这里其实deberta是缺少绝对位置信息的,可是绝对位置还是有用的,所以作者表示我们还是得加上绝对位置信息.</p>
<blockquote>
<p>The BERT model incorporates absolute positions in the input layer. In DeBERTa, we incorporate them right after all the Transformer layers but before the softmax layer for masked token prediction.</p>
</blockquote>
<p>所以说白了就是解码的时候(这里的解码个人感觉表示的其实是predict the masked word的意思)我们会加入absolute position来作为补充信息.</p>
<h2 id="scale-invariant-fine-tuningsift">SCALE INVARIANT FINE-TUNING(SIFT)</h2>
<p>对抗训练一般用来让模型更robust, 在nlp中比较合理的加入干扰的位置应该是在word embedding. 但是embedding本身范围并没有做normalization, 所以这里作者提出先对embedding做layernorm,然后再对embedding做干扰.</p>
<h1 id="deberta-feature-engineering-lgbm">DeBERTa + Feature Engineering + LGBM</h1>
<h2 id="import">import</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> polars <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> softmax</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV, RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB,MultinomialNB,ComplementNB</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer,TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> imblearn.ensemble <span class="keyword">import</span> BalancedBaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, ConfusionMatrixDisplay</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, ConfusionMatrixDisplay, f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> cohen_kappa_score</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> log_evaluation, early_stopping</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line">nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="baseline-deberta">baseline DeBERTa</h2>
<p>写了一长串, MODEL_PATHS里面是作者已经训练好了的DeBERTa, load进来, 多模型预测丢prediction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">MAX_LENGTH = <span class="number">1024</span></span><br><span class="line">TEST_DATA_PATH = <span class="string">&quot;/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    AutoModelForSequenceClassification,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">    Trainer,</span><br><span class="line">    DataCollatorWithPadding,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> softmax</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">MODEL_PATHS = [</span><br><span class="line">    <span class="string">&#x27;/kaggle/input/aes2-400-20240419134941/*/*&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;/kaggle/input/best-model-1/deberta-large-fold1/checkpoint-100/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;/kaggle/input/train-best-model-3/deberta-large-fold1/checkpoint-200/&#x27;</span></span><br><span class="line">]</span><br><span class="line">EVAL_BATCH_SIZE = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> MODEL_PATHS:</span><br><span class="line">    models.extend(glob.glob(path))</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(models[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">sample</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(sample[<span class="string">&#x27;full_text&#x27;</span>], max_length=MAX_LENGTH, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df_test = pd.read_csv(TEST_DATA_PATH)</span><br><span class="line">ds = Dataset.from_pandas(df_test).<span class="built_in">map</span>(tokenize).remove_columns([<span class="string">&#x27;essay_id&#x27;</span>, <span class="string">&#x27;full_text&#x27;</span>])</span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;.&quot;</span>, </span><br><span class="line">    per_device_eval_batch_size=EVAL_BATCH_SIZE, </span><br><span class="line">    report_to=<span class="string">&quot;none&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model = AutoModelForSequenceClassification.from_pretrained(model)</span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model=model, </span><br><span class="line">        args=args, </span><br><span class="line">        data_collator=DataCollatorWithPadding(tokenizer), </span><br><span class="line">        tokenizer=tokenizer</span><br><span class="line">    )    </span><br><span class="line">    preds = trainer.predict(ds).predictions</span><br><span class="line">    predictions.append(softmax(preds, axis=-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">del</span> model, trainer</span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line">    gc.collect()</span><br></pre></td></tr></table></figure>
<p>提取DeBERTa的prediction</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">predicted_score = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> predictions:</span><br><span class="line">    predicted_score += p</span><br><span class="line">    </span><br><span class="line">predicted_score /= <span class="built_in">len</span>(predictions)</span><br><span class="line">df_test[<span class="string">&#x27;score&#x27;</span>] = predicted_score.argmax(-<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">df_test.head()</span><br></pre></td></tr></table></figure>
<p><img src="deberta_res.png" /></p>
<h2 id="feature-engineering">Feature Engineering</h2>
<h3 id="text---paragraph">1. text -&gt; paragraph</h3>
<p>依据换行划分自然段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">columns = [  </span><br><span class="line">    (</span><br><span class="line">        pl.col(<span class="string">&quot;full_text&quot;</span>).<span class="built_in">str</span>.split(by=<span class="string">&quot;\n\n&quot;</span>).alias(<span class="string">&quot;paragraph&quot;</span>)</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">PATH = <span class="string">&quot;/kaggle/input/learning-agency-lab-automated-essay-scoring-2/&quot;</span></span><br><span class="line"></span><br><span class="line">train = pl.read_csv(PATH + <span class="string">&quot;train.csv&quot;</span>).with_columns(columns)</span><br><span class="line">test = pl.read_csv(PATH + <span class="string">&quot;test.csv&quot;</span>).with_columns(columns)</span><br><span class="line"></span><br><span class="line">train.head(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="data-preprocessing">2. data preprocessing</h3>
<p>具体包括了:</p>
<ol type="1">
<li>缩写展开</li>
<li>html移除</li>
<li>无用信息比如 @ 和标点符号的移除</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">cList = &#123;</span><br><span class="line">  <span class="string">&quot;ain&#x27;t&quot;</span>: <span class="string">&quot;am not&quot;</span>,<span class="string">&quot;aren&#x27;t&quot;</span>: <span class="string">&quot;are not&quot;</span>,<span class="string">&quot;can&#x27;t&quot;</span>: <span class="string">&quot;cannot&quot;</span>,<span class="string">&quot;can&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;cannot have&quot;</span>,<span class="string">&quot;&#x27;cause&quot;</span>: <span class="string">&quot;because&quot;</span>,  <span class="string">&quot;could&#x27;ve&quot;</span>: <span class="string">&quot;could have&quot;</span>,<span class="string">&quot;couldn&#x27;t&quot;</span>: <span class="string">&quot;could not&quot;</span>,<span class="string">&quot;couldn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;could not have&quot;</span>,<span class="string">&quot;didn&#x27;t&quot;</span>: <span class="string">&quot;did not&quot;</span>,<span class="string">&quot;doesn&#x27;t&quot;</span>: <span class="string">&quot;does not&quot;</span>,<span class="string">&quot;don&#x27;t&quot;</span>: <span class="string">&quot;do not&quot;</span>,<span class="string">&quot;hadn&#x27;t&quot;</span>: <span class="string">&quot;had not&quot;</span>,<span class="string">&quot;hadn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;had not have&quot;</span>,<span class="string">&quot;hasn&#x27;t&quot;</span>: <span class="string">&quot;has not&quot;</span>,</span><br><span class="line">  <span class="string">&quot;haven&#x27;t&quot;</span>: <span class="string">&quot;have not&quot;</span>,<span class="string">&quot;he&#x27;d&quot;</span>: <span class="string">&quot;he would&quot;</span>,<span class="string">&quot;he&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;he would have&quot;</span>,<span class="string">&quot;he&#x27;ll&quot;</span>: <span class="string">&quot;he will&quot;</span>,<span class="string">&quot;he&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;he will have&quot;</span>,<span class="string">&quot;he&#x27;s&quot;</span>: <span class="string">&quot;he is&quot;</span>,</span><br><span class="line">  <span class="string">&quot;how&#x27;d&quot;</span>: <span class="string">&quot;how did&quot;</span>,<span class="string">&quot;how&#x27;d&#x27;y&quot;</span>: <span class="string">&quot;how do you&quot;</span>,<span class="string">&quot;how&#x27;ll&quot;</span>: <span class="string">&quot;how will&quot;</span>,<span class="string">&quot;how&#x27;s&quot;</span>: <span class="string">&quot;how is&quot;</span>,<span class="string">&quot;I&#x27;d&quot;</span>: <span class="string">&quot;I would&quot;</span>,<span class="string">&quot;I&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;I would have&quot;</span>,<span class="string">&quot;I&#x27;ll&quot;</span>: <span class="string">&quot;I will&quot;</span>,<span class="string">&quot;I&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;I will have&quot;</span>,<span class="string">&quot;I&#x27;m&quot;</span>: <span class="string">&quot;I am&quot;</span>,<span class="string">&quot;I&#x27;ve&quot;</span>: <span class="string">&quot;I have&quot;</span>,</span><br><span class="line">  <span class="string">&quot;isn&#x27;t&quot;</span>: <span class="string">&quot;is not&quot;</span>,<span class="string">&quot;it&#x27;d&quot;</span>: <span class="string">&quot;it had&quot;</span>,<span class="string">&quot;it&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;it would have&quot;</span>,<span class="string">&quot;it&#x27;ll&quot;</span>: <span class="string">&quot;it will&quot;</span>, <span class="string">&quot;it&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;it will have&quot;</span>,<span class="string">&quot;it&#x27;s&quot;</span>: <span class="string">&quot;it is&quot;</span>,<span class="string">&quot;let&#x27;s&quot;</span>: <span class="string">&quot;let us&quot;</span>,<span class="string">&quot;ma&#x27;am&quot;</span>: <span class="string">&quot;madam&quot;</span>,<span class="string">&quot;mayn&#x27;t&quot;</span>: <span class="string">&quot;may not&quot;</span>,</span><br><span class="line">  <span class="string">&quot;might&#x27;ve&quot;</span>: <span class="string">&quot;might have&quot;</span>,<span class="string">&quot;mightn&#x27;t&quot;</span>: <span class="string">&quot;might not&quot;</span>,<span class="string">&quot;mightn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;might not have&quot;</span>,<span class="string">&quot;must&#x27;ve&quot;</span>: <span class="string">&quot;must have&quot;</span>,<span class="string">&quot;mustn&#x27;t&quot;</span>: <span class="string">&quot;must not&quot;</span>,<span class="string">&quot;mustn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;must not have&quot;</span>,<span class="string">&quot;needn&#x27;t&quot;</span>: <span class="string">&quot;need not&quot;</span>,<span class="string">&quot;needn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;need not have&quot;</span>,<span class="string">&quot;o&#x27;clock&quot;</span>: <span class="string">&quot;of the clock&quot;</span>,<span class="string">&quot;oughtn&#x27;t&quot;</span>: <span class="string">&quot;ought not&quot;</span>,<span class="string">&quot;oughtn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;ought not have&quot;</span>,<span class="string">&quot;shan&#x27;t&quot;</span>: <span class="string">&quot;shall not&quot;</span>,<span class="string">&quot;sha&#x27;n&#x27;t&quot;</span>: <span class="string">&quot;shall not&quot;</span>,</span><br><span class="line">  <span class="string">&quot;shan&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;shall not have&quot;</span>,<span class="string">&quot;she&#x27;d&quot;</span>: <span class="string">&quot;she would&quot;</span>,<span class="string">&quot;she&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;she would have&quot;</span>,<span class="string">&quot;she&#x27;ll&quot;</span>: <span class="string">&quot;she will&quot;</span>,<span class="string">&quot;she&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;she will have&quot;</span>,<span class="string">&quot;she&#x27;s&quot;</span>: <span class="string">&quot;she is&quot;</span>,</span><br><span class="line">  <span class="string">&quot;should&#x27;ve&quot;</span>: <span class="string">&quot;should have&quot;</span>,<span class="string">&quot;shouldn&#x27;t&quot;</span>: <span class="string">&quot;should not&quot;</span>,<span class="string">&quot;shouldn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;should not have&quot;</span>,<span class="string">&quot;so&#x27;ve&quot;</span>: <span class="string">&quot;so have&quot;</span>,<span class="string">&quot;so&#x27;s&quot;</span>: <span class="string">&quot;so is&quot;</span>,<span class="string">&quot;that&#x27;d&quot;</span>: <span class="string">&quot;that would&quot;</span>,<span class="string">&quot;that&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;that would have&quot;</span>,<span class="string">&quot;that&#x27;s&quot;</span>: <span class="string">&quot;that is&quot;</span>,<span class="string">&quot;there&#x27;d&quot;</span>: <span class="string">&quot;there had&quot;</span>,<span class="string">&quot;there&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;there would have&quot;</span>,<span class="string">&quot;there&#x27;s&quot;</span>: <span class="string">&quot;there is&quot;</span>,<span class="string">&quot;they&#x27;d&quot;</span>: <span class="string">&quot;they would&quot;</span>,<span class="string">&quot;they&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;they would have&quot;</span>,<span class="string">&quot;they&#x27;ll&quot;</span>: <span class="string">&quot;they will&quot;</span>,<span class="string">&quot;they&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;they will have&quot;</span>,<span class="string">&quot;they&#x27;re&quot;</span>: <span class="string">&quot;they are&quot;</span>,<span class="string">&quot;they&#x27;ve&quot;</span>: <span class="string">&quot;they have&quot;</span>,<span class="string">&quot;to&#x27;ve&quot;</span>: <span class="string">&quot;to have&quot;</span>,<span class="string">&quot;wasn&#x27;t&quot;</span>: <span class="string">&quot;was not&quot;</span>,<span class="string">&quot;we&#x27;d&quot;</span>: <span class="string">&quot;we had&quot;</span>,</span><br><span class="line">  <span class="string">&quot;we&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;we would have&quot;</span>,<span class="string">&quot;we&#x27;ll&quot;</span>: <span class="string">&quot;we will&quot;</span>,<span class="string">&quot;we&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;we will have&quot;</span>,<span class="string">&quot;we&#x27;re&quot;</span>: <span class="string">&quot;we are&quot;</span>,<span class="string">&quot;we&#x27;ve&quot;</span>: <span class="string">&quot;we have&quot;</span>,</span><br><span class="line">  <span class="string">&quot;weren&#x27;t&quot;</span>: <span class="string">&quot;were not&quot;</span>,<span class="string">&quot;what&#x27;ll&quot;</span>: <span class="string">&quot;what will&quot;</span>,<span class="string">&quot;what&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;what will have&quot;</span>,</span><br><span class="line">  <span class="string">&quot;what&#x27;re&quot;</span>: <span class="string">&quot;what are&quot;</span>,<span class="string">&quot;what&#x27;s&quot;</span>: <span class="string">&quot;what is&quot;</span>,<span class="string">&quot;what&#x27;ve&quot;</span>: <span class="string">&quot;what have&quot;</span>,<span class="string">&quot;when&#x27;s&quot;</span>: <span class="string">&quot;when is&quot;</span>,<span class="string">&quot;when&#x27;ve&quot;</span>: <span class="string">&quot;when have&quot;</span>,</span><br><span class="line">  <span class="string">&quot;where&#x27;d&quot;</span>: <span class="string">&quot;where did&quot;</span>,<span class="string">&quot;where&#x27;s&quot;</span>: <span class="string">&quot;where is&quot;</span>,<span class="string">&quot;where&#x27;ve&quot;</span>: <span class="string">&quot;where have&quot;</span>,<span class="string">&quot;who&#x27;ll&quot;</span>: <span class="string">&quot;who will&quot;</span>,<span class="string">&quot;who&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;who will have&quot;</span>,<span class="string">&quot;who&#x27;s&quot;</span>: <span class="string">&quot;who is&quot;</span>,<span class="string">&quot;who&#x27;ve&quot;</span>: <span class="string">&quot;who have&quot;</span>,<span class="string">&quot;why&#x27;s&quot;</span>: <span class="string">&quot;why is&quot;</span>,</span><br><span class="line">  <span class="string">&quot;why&#x27;ve&quot;</span>: <span class="string">&quot;why have&quot;</span>,<span class="string">&quot;will&#x27;ve&quot;</span>: <span class="string">&quot;will have&quot;</span>,<span class="string">&quot;won&#x27;t&quot;</span>: <span class="string">&quot;will not&quot;</span>,<span class="string">&quot;won&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;will not have&quot;</span>,<span class="string">&quot;would&#x27;ve&quot;</span>: <span class="string">&quot;would have&quot;</span>,<span class="string">&quot;wouldn&#x27;t&quot;</span>: <span class="string">&quot;would not&quot;</span>,</span><br><span class="line">  <span class="string">&quot;wouldn&#x27;t&#x27;ve&quot;</span>: <span class="string">&quot;would not have&quot;</span>,<span class="string">&quot;y&#x27;all&quot;</span>: <span class="string">&quot;you all&quot;</span>,<span class="string">&quot;y&#x27;alls&quot;</span>: <span class="string">&quot;you alls&quot;</span>,<span class="string">&quot;y&#x27;all&#x27;d&quot;</span>: <span class="string">&quot;you all would&quot;</span>,</span><br><span class="line">  <span class="string">&quot;y&#x27;all&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;you all would have&quot;</span>,<span class="string">&quot;y&#x27;all&#x27;re&quot;</span>: <span class="string">&quot;you all are&quot;</span>,<span class="string">&quot;y&#x27;all&#x27;ve&quot;</span>: <span class="string">&quot;you all have&quot;</span>,<span class="string">&quot;you&#x27;d&quot;</span>: <span class="string">&quot;you had&quot;</span>,<span class="string">&quot;you&#x27;d&#x27;ve&quot;</span>: <span class="string">&quot;you would have&quot;</span>,<span class="string">&quot;you&#x27;ll&quot;</span>: <span class="string">&quot;you you will&quot;</span>,<span class="string">&quot;you&#x27;ll&#x27;ve&quot;</span>: <span class="string">&quot;you you will have&quot;</span>,<span class="string">&quot;you&#x27;re&quot;</span>: <span class="string">&quot;you are&quot;</span>,  <span class="string">&quot;you&#x27;ve&quot;</span>: <span class="string">&quot;you have&quot;</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">c_re = re.<span class="built_in">compile</span>(<span class="string">&#x27;(%s)&#x27;</span> % <span class="string">&#x27;|&#x27;</span>.join(cList.keys()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">expandContractions</span>(<span class="params">text, c_re=c_re</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">replace</span>(<span class="params"><span class="keyword">match</span></span>):</span><br><span class="line">        <span class="keyword">return</span> cList[<span class="keyword">match</span>.group(<span class="number">0</span>)]</span><br><span class="line">    <span class="keyword">return</span> c_re.sub(replace, text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">removeHTML</span>(<span class="params">x</span>):</span><br><span class="line">    html=re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;.*?&gt;&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> html.sub(<span class="string">r&#x27;&#x27;</span>,x)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataPreprocessing</span>(<span class="params">x</span>):</span><br><span class="line">    x = x.lower()</span><br><span class="line">    x = removeHTML(x)</span><br><span class="line">    x = re.sub(<span class="string">&quot;@\w+&quot;</span>, <span class="string">&#x27;&#x27;</span>,x)</span><br><span class="line">    x = re.sub(<span class="string">&quot;&#x27;\d+&quot;</span>, <span class="string">&#x27;&#x27;</span>,x)</span><br><span class="line">    x = re.sub(<span class="string">&quot;\d+&quot;</span>, <span class="string">&#x27;&#x27;</span>,x)</span><br><span class="line">    x = re.sub(<span class="string">&quot;http\w+&quot;</span>, <span class="string">&#x27;&#x27;</span>,x)</span><br><span class="line">    x = re.sub(<span class="string">r&quot;\s+&quot;</span>, <span class="string">&quot; &quot;</span>, x)</span><br><span class="line"><span class="comment">#     x = expandContractions(x)</span></span><br><span class="line">    x = re.sub(<span class="string">r&quot;\.+&quot;</span>, <span class="string">&quot;.&quot;</span>, x)</span><br><span class="line">    x = re.sub(<span class="string">r&quot;\,+&quot;</span>, <span class="string">&quot;,&quot;</span>, x)</span><br><span class="line">    x = x.strip()</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_punctuation</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Remove all punctuation from the input text.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    - text (str): The input text.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - str: The text with punctuation removed.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    translator = <span class="built_in">str</span>.maketrans(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>, string.punctuation)</span><br><span class="line">    <span class="keyword">return</span> text.translate(translator)</span><br></pre></td></tr></table></figure>
<p>很专业好吧,这种缩写展开的词典肯定是经常做这份工作才能有的, 就像pentest选手人手password爆破字典一样.</p>
<h3 id="count-error">3. count error</h3>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/xianhellg/english-word-hx">english-word-hx地址</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/kaggle/input/english-word-hx/words.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    english_vocab = <span class="built_in">set</span>(word.strip().lower() <span class="keyword">for</span> word <span class="keyword">in</span> file)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_spelling_errors</span>(<span class="params">text</span>):</span><br><span class="line">    doc = nlp(text)</span><br><span class="line">    lemmatized_tokens = [token.lemma_.lower() <span class="keyword">for</span> token <span class="keyword">in</span> doc]</span><br><span class="line">    spelling_errors = <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> token <span class="keyword">in</span> lemmatized_tokens <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> english_vocab)</span><br><span class="line">    <span class="keyword">return</span> spelling_errors</span><br></pre></td></tr></table></figure>
<p>这里就是依据英语单词字典, 将作文中每个词映射到词根然后判断词根是否出现在字典中, 一旦不在字典中就是拼写错误.</p>
<h3 id="第一波处理paragraph">第一波处理(paragraph)</h3>
<ol type="1">
<li><p>预处理, 移除标点, 计算拼写错误, 计算自然段长度, 计算每个句子长度.</p></li>
<li><p>设置两个划分list, 统计自然段长度大于xx作为一个feature, 小于多少作为一个feature</p></li>
<li><p>一般特征加上拼写错误数量: 统计均值,最大,最小值,峰值等等一些统计学数据.</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Paragraph_Preprocess</span>(<span class="params">tmp</span>):</span><br><span class="line"></span><br><span class="line">    tmp = tmp.explode(<span class="string">&#x27;paragraph&#x27;</span>)</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;paragraph&#x27;</span>).map_elements(dataPreprocessing))</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;paragraph&#x27;</span>).map_elements(remove_punctuation).alias(<span class="string">&#x27;paragraph_no_pinctuation&#x27;</span>))</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;paragraph_no_pinctuation&#x27;</span>).map_elements(count_spelling_errors).alias(<span class="string">&quot;paragraph_error_num&quot;</span>))</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;paragraph&#x27;</span>).map_elements(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)).alias(<span class="string">&quot;paragraph_len&quot;</span>))</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;paragraph&#x27;</span>).map_elements(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27;.&#x27;</span>))).alias(<span class="string">&quot;paragraph_sentence_cnt&quot;</span>),</span><br><span class="line">                    pl.col(<span class="string">&#x27;paragraph&#x27;</span>).map_elements(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27; &#x27;</span>))).alias(<span class="string">&quot;paragraph_word_cnt&quot;</span>),)</span><br><span class="line">    <span class="keyword">return</span> tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># feature_eng</span></span><br><span class="line">paragraph_fea = [<span class="string">&#x27;paragraph_len&#x27;</span>,<span class="string">&#x27;paragraph_sentence_cnt&#x27;</span>,<span class="string">&#x27;paragraph_word_cnt&#x27;</span>]</span><br><span class="line">paragraph_fea2 = [<span class="string">&#x27;paragraph_error_num&#x27;</span>] + paragraph_fea</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Paragraph_Eng</span>(<span class="params">train_tmp</span>):</span><br><span class="line">    num_list = [<span class="number">0</span>, <span class="number">50</span>,<span class="number">75</span>,<span class="number">100</span>,<span class="number">125</span>,<span class="number">150</span>,<span class="number">175</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">500</span>,<span class="number">600</span>]</span><br><span class="line">    num_list2 = [<span class="number">0</span>, <span class="number">50</span>,<span class="number">75</span>,<span class="number">100</span>,<span class="number">125</span>,<span class="number">150</span>,<span class="number">175</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">500</span>,<span class="number">600</span>,<span class="number">700</span>]</span><br><span class="line">    aggs = [</span><br><span class="line">        *[pl.col(<span class="string">&#x27;paragraph&#x27;</span>).<span class="built_in">filter</span>(pl.col(<span class="string">&#x27;paragraph_len&#x27;</span>) &gt;= i).count().alias(<span class="string">f&quot;paragraph_&gt;<span class="subst">&#123;i&#125;</span>_cnt&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">50</span>,<span class="number">75</span>,<span class="number">100</span>,<span class="number">125</span>,<span class="number">150</span>,<span class="number">175</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">500</span>,<span class="number">600</span>,<span class="number">700</span>] ], </span><br><span class="line">        *[pl.col(<span class="string">&#x27;paragraph&#x27;</span>).<span class="built_in">filter</span>(pl.col(<span class="string">&#x27;paragraph_len&#x27;</span>) &lt;= i).count().alias(<span class="string">f&quot;paragraph_&lt;<span class="subst">&#123;i&#125;</span>_cnt&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">25</span>,<span class="number">49</span>]], </span><br><span class="line">        *[pl.col(fea).<span class="built_in">max</span>().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_max&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).mean().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_mean&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).<span class="built_in">min</span>().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_min&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).<span class="built_in">sum</span>().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_sum&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).first().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_first&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).last().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_last&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).kurtosis().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_kurtosis&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).quantile(<span class="number">0.25</span>).alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_q1&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        *[pl.col(fea).quantile(<span class="number">0.75</span>).alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_q3&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> paragraph_fea2],</span><br><span class="line">        ]</span><br><span class="line">    df = train_tmp.group_by([<span class="string">&#x27;essay_id&#x27;</span>], maintain_order=<span class="literal">True</span>).agg(aggs).sort(<span class="string">&quot;essay_id&quot;</span>)</span><br><span class="line">    df = df.to_pandas()</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line">tmp = Paragraph_Preprocess(train)</span><br><span class="line">train_feats = Paragraph_Eng(tmp)</span><br><span class="line">train_feats[<span class="string">&#x27;score&#x27;</span>] = train[<span class="string">&#x27;score&#x27;</span>]</span><br><span class="line"></span><br><span class="line">feature_names = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;essay_id&#x27;</span>,<span class="string">&#x27;score&#x27;</span>], train_feats.columns))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Features Number: &#x27;</span>,<span class="built_in">len</span>(feature_names))</span><br><span class="line">train_feats.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features Number:  53</span></span><br></pre></td></tr></table></figure>
<p>这时候特征数量来到了53</p>
<h3 id="第二波处理sentence">第二波处理(sentence)</h3>
<p>类似于paragraph</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Sentence_Preprocess</span>(<span class="params">tmp</span>):</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;full_text&#x27;</span>).map_elements(dataPreprocessing).<span class="built_in">str</span>.split(by=<span class="string">&quot;.&quot;</span>).alias(<span class="string">&quot;sentence&quot;</span>))</span><br><span class="line">    tmp = tmp.explode(<span class="string">&#x27;sentence&#x27;</span>)</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;sentence&#x27;</span>).map_elements(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)).alias(<span class="string">&quot;sentence_len&quot;</span>))</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;sentence&#x27;</span>).map_elements(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27; &#x27;</span>))).alias(<span class="string">&quot;sentence_word_cnt&quot;</span>))    </span><br><span class="line">    <span class="keyword">return</span> tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># feature_eng</span></span><br><span class="line">sentence_fea = [<span class="string">&#x27;sentence_len&#x27;</span>,<span class="string">&#x27;sentence_word_cnt&#x27;</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Sentence_Eng</span>(<span class="params">train_tmp</span>):</span><br><span class="line">    aggs = [</span><br><span class="line">        *[pl.col(<span class="string">&#x27;sentence&#x27;</span>).<span class="built_in">filter</span>(pl.col(<span class="string">&#x27;sentence_len&#x27;</span>) &gt;= i).count().alias(<span class="string">f&quot;sentence_&gt;<span class="subst">&#123;i&#125;</span>_cnt&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">15</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>] ], </span><br><span class="line">        *[pl.col(<span class="string">&#x27;sentence&#x27;</span>).<span class="built_in">filter</span>(pl.col(<span class="string">&#x27;sentence_len&#x27;</span>) &lt;= i).count().alias(<span class="string">f&quot;sentence_&lt;<span class="subst">&#123;i&#125;</span>_cnt&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">15</span>,<span class="number">50</span>] ], </span><br><span class="line">        *[pl.col(fea).<span class="built_in">max</span>().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_max&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).mean().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_mean&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).<span class="built_in">min</span>().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_min&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).<span class="built_in">sum</span>().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_sum&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).first().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_first&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).last().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_last&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).kurtosis().alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_kurtosis&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).quantile(<span class="number">0.25</span>).alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_q1&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">        *[pl.col(fea).quantile(<span class="number">0.75</span>).alias(<span class="string">f&quot;<span class="subst">&#123;fea&#125;</span>_q3&quot;</span>) <span class="keyword">for</span> fea <span class="keyword">in</span> sentence_fea],</span><br><span class="line">    </span><br><span class="line">        ]</span><br><span class="line">    df = train_tmp.group_by([<span class="string">&#x27;essay_id&#x27;</span>], maintain_order=<span class="literal">True</span>).agg(aggs).sort(<span class="string">&quot;essay_id&quot;</span>)</span><br><span class="line">    df = df.to_pandas()</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">tmp = Sentence_Preprocess(train)</span><br><span class="line">train_feats = train_feats.merge(Sentence_Eng(tmp), on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line">feature_names = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;essay_id&#x27;</span>,<span class="string">&#x27;score&#x27;</span>], train_feats.columns))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Features Number: &#x27;</span>,<span class="built_in">len</span>(feature_names))</span><br><span class="line">train_feats.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features Number:  81</span></span><br></pre></td></tr></table></figure>
<h3 id="word">word</h3>
<p>然后对单词长度处理,个人感觉有点离谱了,不过就当先了解下可以做的处理了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word feature</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Word_Preprocess</span>(<span class="params">tmp</span>):</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;full_text&#x27;</span>).map_elements(dataPreprocessing).<span class="built_in">str</span>.split(by=<span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;word&quot;</span>))</span><br><span class="line">    tmp = tmp.explode(<span class="string">&#x27;word&#x27;</span>)</span><br><span class="line">    tmp = tmp.with_columns(pl.col(<span class="string">&#x27;word&#x27;</span>).map_elements(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)).alias(<span class="string">&quot;word_len&quot;</span>))</span><br><span class="line">    tmp = tmp.<span class="built_in">filter</span>(pl.col(<span class="string">&#x27;word_len&#x27;</span>)!=<span class="number">0</span>)    </span><br><span class="line">    <span class="keyword">return</span> tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># feature_eng</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Word_Eng</span>(<span class="params">train_tmp</span>):</span><br><span class="line">    aggs = [</span><br><span class="line">        *[pl.col(<span class="string">&#x27;word&#x27;</span>).<span class="built_in">filter</span>(pl.col(<span class="string">&#x27;word_len&#x27;</span>) &gt;= i+<span class="number">1</span>).count().alias(<span class="string">f&quot;word_<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>_cnt&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>) ], </span><br><span class="line">        pl.col(<span class="string">&#x27;word_len&#x27;</span>).<span class="built_in">max</span>().alias(<span class="string">f&quot;word_len_max&quot;</span>),</span><br><span class="line">        pl.col(<span class="string">&#x27;word_len&#x27;</span>).mean().alias(<span class="string">f&quot;word_len_mean&quot;</span>),</span><br><span class="line">        pl.col(<span class="string">&#x27;word_len&#x27;</span>).std().alias(<span class="string">f&quot;word_len_std&quot;</span>),</span><br><span class="line">        pl.col(<span class="string">&#x27;word_len&#x27;</span>).quantile(<span class="number">0.25</span>).alias(<span class="string">f&quot;word_len_q1&quot;</span>),</span><br><span class="line">        pl.col(<span class="string">&#x27;word_len&#x27;</span>).quantile(<span class="number">0.50</span>).alias(<span class="string">f&quot;word_len_q2&quot;</span>),</span><br><span class="line">        pl.col(<span class="string">&#x27;word_len&#x27;</span>).quantile(<span class="number">0.75</span>).alias(<span class="string">f&quot;word_len_q3&quot;</span>),</span><br><span class="line">        ]</span><br><span class="line">    df = train_tmp.group_by([<span class="string">&#x27;essay_id&#x27;</span>], maintain_order=<span class="literal">True</span>).agg(aggs).sort(<span class="string">&quot;essay_id&quot;</span>)</span><br><span class="line">    df = df.to_pandas()</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">tmp = Word_Preprocess(train)</span><br><span class="line">train_feats = train_feats.merge(Word_Eng(tmp), on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line">feature_names = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;essay_id&#x27;</span>,<span class="string">&#x27;score&#x27;</span>], train_feats.columns))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Features Number: &#x27;</span>,<span class="built_in">len</span>(feature_names))</span><br><span class="line">train_feats.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features Number:  102</span></span><br></pre></td></tr></table></figure>
<h3 id="tfidf">tfidf</h3>
<p>真没想到居然还是用得到这个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vectorizer = TfidfVectorizer(</span><br><span class="line">            tokenizer=<span class="keyword">lambda</span> x: x,</span><br><span class="line">            preprocessor=<span class="keyword">lambda</span> x: x,</span><br><span class="line">            token_pattern=<span class="literal">None</span>,</span><br><span class="line">            strip_accents=<span class="string">&#x27;unicode&#x27;</span>,</span><br><span class="line">            analyzer = <span class="string">&#x27;word&#x27;</span>,</span><br><span class="line">            ngram_range=(<span class="number">3</span>,<span class="number">6</span>),</span><br><span class="line">            min_df=<span class="number">0.05</span>,</span><br><span class="line">            max_df=<span class="number">0.95</span>,</span><br><span class="line">            sublinear_tf=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_tfid = vectorizer.fit_transform([i <span class="keyword">for</span> i <span class="keyword">in</span> train[<span class="string">&#x27;full_text&#x27;</span>]])</span><br><span class="line">dense_matrix = train_tfid.toarray()</span><br><span class="line">df = pd.DataFrame(dense_matrix)</span><br><span class="line">tfid_columns = [ <span class="string">f&#x27;tfid_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df.columns))]</span><br><span class="line">df.columns = tfid_columns</span><br><span class="line">df[<span class="string">&#x27;essay_id&#x27;</span>] = train_feats[<span class="string">&#x27;essay_id&#x27;</span>]</span><br><span class="line">train_feats = train_feats.merge(df, on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">feature_names = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;essay_id&#x27;</span>,<span class="string">&#x27;score&#x27;</span>], train_feats.columns))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of Features: &#x27;</span>,<span class="built_in">len</span>(feature_names))</span><br><span class="line">train_feats.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of Features:  19729</span></span><br></pre></td></tr></table></figure>
<p>可以看到特征数量直接开始爆炸, 但tf-idf的老问题还是存在</p>
<p><img src="tf-idf.png" /></p>
<h3 id="count">count</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_cnt = CountVectorizer(</span><br><span class="line">            tokenizer=<span class="keyword">lambda</span> x: x,</span><br><span class="line">            preprocessor=<span class="keyword">lambda</span> x: x,</span><br><span class="line">            token_pattern=<span class="literal">None</span>,</span><br><span class="line">            strip_accents=<span class="string">&#x27;unicode&#x27;</span>,</span><br><span class="line">            analyzer = <span class="string">&#x27;word&#x27;</span>,</span><br><span class="line">            ngram_range=(<span class="number">2</span>,<span class="number">3</span>),</span><br><span class="line">            min_df=<span class="number">0.10</span>,</span><br><span class="line">            max_df=<span class="number">0.85</span>,</span><br><span class="line">)</span><br><span class="line">train_tfid = vectorizer_cnt.fit_transform([i <span class="keyword">for</span> i <span class="keyword">in</span> train[<span class="string">&#x27;full_text&#x27;</span>]])</span><br><span class="line">dense_matrix = train_tfid.toarray()</span><br><span class="line">df = pd.DataFrame(dense_matrix)</span><br><span class="line">tfid_columns = [ <span class="string">f&#x27;tfid_cnt_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df.columns))]</span><br><span class="line">df.columns = tfid_columns</span><br><span class="line">df[<span class="string">&#x27;essay_id&#x27;</span>] = train_feats[<span class="string">&#x27;essay_id&#x27;</span>]</span><br><span class="line">train_feats = train_feats.merge(df, on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="整合deberta">整合deberta</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line">deberta_oof = joblib.load(<span class="string">&#x27;/kaggle/input/aes2-400-20240419134941/oof.pkl&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(deberta_oof.shape, train_feats.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    train_feats[<span class="string">f&#x27;deberta_oof_<span class="subst">&#123;i&#125;</span>&#x27;</span>] = deberta_oof[:, i]</span><br><span class="line"></span><br><span class="line">feature_names = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;essay_id&#x27;</span>,<span class="string">&#x27;score&#x27;</span>], train_feats.columns))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Features Number: &#x27;</span>,<span class="built_in">len</span>(feature_names))    </span><br><span class="line"></span><br><span class="line">train_feats.shape</span><br></pre></td></tr></table></figure>
<h3 id="evaluation-metrics">evaluation metrics</h3>
<p>比赛给出的要求是会看quadratic weighted kappa</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">quadratic_weighted_kappa</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    y_true = y_true + a</span><br><span class="line">    y_pred = (y_pred + a).clip(<span class="number">1</span>, <span class="number">6</span>).<span class="built_in">round</span>()</span><br><span class="line">    qwk = cohen_kappa_score(y_true, y_pred, weights=<span class="string">&quot;quadratic&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;QWK&#x27;</span>, qwk, <span class="literal">True</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">qwk_obj</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    labels = y_true + a</span><br><span class="line">    preds = y_pred + a</span><br><span class="line">    preds = preds.clip(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">    f = <span class="number">1</span>/<span class="number">2</span>*np.<span class="built_in">sum</span>((preds-labels)**<span class="number">2</span>)</span><br><span class="line">    g = <span class="number">1</span>/<span class="number">2</span>*np.<span class="built_in">sum</span>((preds-a)**<span class="number">2</span>+b)</span><br><span class="line">    df = preds - labels</span><br><span class="line">    dg = preds - a</span><br><span class="line">    grad = (df/g - f*dg/g**<span class="number">2</span>)*<span class="built_in">len</span>(labels)</span><br><span class="line">    hess = np.ones(<span class="built_in">len</span>(labels))</span><br><span class="line">    <span class="keyword">return</span> grad, hess</span><br><span class="line">a = <span class="number">2.998</span></span><br><span class="line">b = <span class="number">1.092</span></span><br></pre></td></tr></table></figure>
<h3 id="整合数据丢lgbm">整合数据,丢LGBM</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/kaggle/input/aes2-400-fes-202404291649/usefe_list.pkl&#x27;</span>, mode=<span class="string">&#x27;br&#x27;</span>) <span class="keyword">as</span> fi:</span><br><span class="line">  feature_names = pickle.load(fi)</span><br><span class="line">feature_select = feature_names</span><br><span class="line"></span><br><span class="line">X = train_feats[feature_names].astype(np.float32).values</span><br><span class="line">y_split = train_feats[<span class="string">&#x27;score&#x27;</span>].astype(<span class="built_in">int</span>).values</span><br><span class="line">y = train_feats[<span class="string">&#x27;score&#x27;</span>].astype(np.float32).values-a</span><br><span class="line">oof = train_feats[<span class="string">&#x27;score&#x27;</span>].astype(<span class="built_in">int</span>).values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_splits = <span class="number">15</span></span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=n_splits, shuffle=<span class="literal">True</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">f1_scores = []</span><br><span class="line">kappa_scores = []</span><br><span class="line">models = []</span><br><span class="line">predictions = []</span><br><span class="line">callbacks = [log_evaluation(period=<span class="number">25</span>), early_stopping(stopping_rounds=<span class="number">75</span>,first_metric_only=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line">i=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> skf.split(X, y_split):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;fold&#x27;</span>,i)</span><br><span class="line">    X_train_fold, X_test_fold = X[train_index], X[test_index]   </span><br><span class="line">    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]</span><br><span class="line">    model = lgb.LGBMRegressor(</span><br><span class="line">                objective = qwk_obj,</span><br><span class="line">                metrics = <span class="string">&#x27;None&#x27;</span>,</span><br><span class="line">                learning_rate = <span class="number">0.05</span>,</span><br><span class="line">                max_depth = <span class="number">5</span>,</span><br><span class="line">                num_leaves = <span class="number">10</span>,</span><br><span class="line">                colsample_bytree=<span class="number">0.3</span>,</span><br><span class="line">                reg_alpha = <span class="number">0.7</span>,</span><br><span class="line">                reg_lambda = <span class="number">0.1</span>,</span><br><span class="line">                n_estimators=<span class="number">700</span>,</span><br><span class="line">                random_state=<span class="number">42</span>,</span><br><span class="line">                extra_trees=<span class="literal">True</span>,</span><br><span class="line">                class_weight=<span class="string">&#x27;balanced&#x27;</span>,</span><br><span class="line">                verbosity = - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    predictor = model.fit(X_train_fold,</span><br><span class="line">                                  y_train_fold,</span><br><span class="line">                                  eval_names=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>],</span><br><span class="line">                                  eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],</span><br><span class="line">                                  eval_metric=quadratic_weighted_kappa,</span><br><span class="line">                                  callbacks=callbacks,)</span><br><span class="line">    models.append(predictor)</span><br><span class="line">    predictions_fold = predictor.predict(X_test_fold)</span><br><span class="line">    predictions_fold = predictions_fold + a</span><br><span class="line">    oof[test_index]=predictions_fold</span><br><span class="line">    predictions_fold = predictions_fold.clip(<span class="number">1</span>, <span class="number">6</span>).<span class="built_in">round</span>()</span><br><span class="line">    predictions.append(predictions_fold)</span><br><span class="line">    f1_fold = f1_score(y_test_fold_int, predictions_fold, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">    f1_scores.append(f1_fold)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights=<span class="string">&#x27;quadratic&#x27;</span>)</span><br><span class="line">    kappa_scores.append(kappa_fold)</span><br><span class="line">    </span><br><span class="line">    cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">7</span>)])</span><br><span class="line"></span><br><span class="line">    disp = ConfusionMatrixDisplay(confusion_matrix=cm,</span><br><span class="line">                                  display_labels=[x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">7</span>)])</span><br><span class="line">    disp.plot()</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;F1 score across fold: <span class="subst">&#123;f1_fold&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Cohen kappa score across fold: <span class="subst">&#123;kappa_fold&#125;</span>&#x27;</span>)</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">mean_f1_score = np.mean(f1_scores)</span><br><span class="line">mean_kappa_score = np.mean(kappa_scores)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Mean F1 score across <span class="subst">&#123;n_splits&#125;</span> folds: <span class="subst">&#123;mean_f1_score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Mean Cohen kappa score across <span class="subst">&#123;n_splits&#125;</span> folds: <span class="subst">&#123;mean_kappa_score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h3 id="test-data-prediction">test data prediction</h3>
<p>预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">tmp = Paragraph_Preprocess(test)</span><br><span class="line">test_feats = Paragraph_Eng(tmp)</span><br><span class="line"><span class="comment"># Sentence</span></span><br><span class="line">tmp = Sentence_Preprocess(test)</span><br><span class="line">test_feats = test_feats.merge(Sentence_Eng(tmp), on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"><span class="comment"># Word</span></span><br><span class="line">tmp = Word_Preprocess(test)</span><br><span class="line">test_feats = test_feats.merge(Word_Eng(tmp), on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tfidf</span></span><br><span class="line">test_tfid = vectorizer.transform([i <span class="keyword">for</span> i <span class="keyword">in</span> test[<span class="string">&#x27;full_text&#x27;</span>]])</span><br><span class="line">dense_matrix = test_tfid.toarray()</span><br><span class="line">df = pd.DataFrame(dense_matrix)</span><br><span class="line">tfid_columns = [ <span class="string">f&#x27;tfid_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df.columns))]</span><br><span class="line">df.columns = tfid_columns</span><br><span class="line">df[<span class="string">&#x27;essay_id&#x27;</span>] = test_feats[<span class="string">&#x27;essay_id&#x27;</span>]</span><br><span class="line">test_feats = test_feats.merge(df, on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CountVectorizer</span></span><br><span class="line">test_tfid = vectorizer_cnt.transform([i <span class="keyword">for</span> i <span class="keyword">in</span> test[<span class="string">&#x27;full_text&#x27;</span>]])</span><br><span class="line">dense_matrix = test_tfid.toarray()</span><br><span class="line">df = pd.DataFrame(dense_matrix)</span><br><span class="line">tfid_columns = [ <span class="string">f&#x27;tfid_cnt_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df.columns))]</span><br><span class="line">df.columns = tfid_columns</span><br><span class="line">df[<span class="string">&#x27;essay_id&#x27;</span>] = test_feats[<span class="string">&#x27;essay_id&#x27;</span>]</span><br><span class="line">test_feats = test_feats.merge(df, on=<span class="string">&#x27;essay_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    test_feats[<span class="string">f&#x27;deberta_oof_<span class="subst">&#123;i&#125;</span>&#x27;</span>] = predicted_score[:, i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features number</span></span><br><span class="line">feature_names = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;essay_id&#x27;</span>,<span class="string">&#x27;score&#x27;</span>], test_feats.columns))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Features number: &#x27;</span>,<span class="built_in">len</span>(feature_names))</span><br><span class="line">test_feats.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>计算概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">probabilities = []</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    proba= model.predict(test_feats[feature_select])+ a</span><br><span class="line">    probabilities.append(proba)</span><br><span class="line"></span><br><span class="line">predictions = np.mean(probabilities, axis=<span class="number">0</span>)</span><br><span class="line">predictions = np.<span class="built_in">round</span>(predictions.clip(<span class="number">1</span>, <span class="number">6</span>))</span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br><span class="line"></span><br><span class="line">submission=pd.read_csv(<span class="string">&quot;/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv&quot;</span>)</span><br><span class="line">submission[<span class="string">&#x27;score&#x27;</span>]=predictions</span><br><span class="line">submission[<span class="string">&#x27;score&#x27;</span>]=submission[<span class="string">&#x27;score&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line">submission.to_csv(<span class="string">&quot;submission.csv&quot;</span>,index=<span class="literal">None</span>)</span><br><span class="line">display(submission.head())</span><br></pre></td></tr></table></figure>
<h3 id="update2024.05.22">update(2024.05.22)</h3>
<p>最新跟进, 可以调用另外一个competition的notebook来继续做feature extraction. 另一个notebook是<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/kojimar/fb3-deberta-family-inference-9-28-updated">FB3 Deberta Family Inference [9/28 UPDATED]</a></p>
<p>这个notebook做了文本的连贯性,词汇量,语法等层面的处理, 这里直接调用FB模型跑AES的train_data, 然后生成的数据作为feature+DeBERTa+LGBM做进一步学习, train结束后对testdata也做FB处理然后再做预测. 最后分数从0.82提升到了0.822.</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#deberta"><span class="toc-number">1.</span> <span class="toc-text">DeBERTa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#disentangled-attention"><span class="toc-number">1.1.</span> <span class="toc-text">Disentangled Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#enhanced-mask-decoder"><span class="toc-number">1.2.</span> <span class="toc-text">Enhanced Mask Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scale-invariant-fine-tuningsift"><span class="toc-number">1.3.</span> <span class="toc-text">SCALE INVARIANT FINE-TUNING(SIFT)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#deberta-feature-engineering-lgbm"><span class="toc-number">2.</span> <span class="toc-text">DeBERTa + Feature Engineering + LGBM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#import"><span class="toc-number">2.1.</span> <span class="toc-text">import</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#baseline-deberta"><span class="toc-number">2.2.</span> <span class="toc-text">baseline DeBERTa</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-engineering"><span class="toc-number">2.3.</span> <span class="toc-text">Feature Engineering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#text---paragraph"><span class="toc-number">2.3.1.</span> <span class="toc-text">1. text -&gt; paragraph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#data-preprocessing"><span class="toc-number">2.3.2.</span> <span class="toc-text">2. data preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count-error"><span class="toc-number">2.3.3.</span> <span class="toc-text">3. count error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%B3%A2%E5%A4%84%E7%90%86paragraph"><span class="toc-number">2.3.4.</span> <span class="toc-text">第一波处理(paragraph)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%B3%A2%E5%A4%84%E7%90%86sentence"><span class="toc-number">2.3.5.</span> <span class="toc-text">第二波处理(sentence)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#word"><span class="toc-number">2.3.6.</span> <span class="toc-text">word</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tfidf"><span class="toc-number">2.3.7.</span> <span class="toc-text">tfidf</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count"><span class="toc-number">2.3.8.</span> <span class="toc-text">count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88deberta"><span class="toc-number">2.3.9.</span> <span class="toc-text">整合deberta</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#evaluation-metrics"><span class="toc-number">2.3.10.</span> <span class="toc-text">evaluation metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E6%95%B0%E6%8D%AE%E4%B8%A2lgbm"><span class="toc-number">2.3.11.</span> <span class="toc-text">整合数据,丢LGBM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#test-data-prediction"><span class="toc-number">2.3.12.</span> <span class="toc-text">test data prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#update2024.05.22"><span class="toc-number">2.3.13.</span> <span class="toc-text">update(2024.05.22)</span></a></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&text=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&is_video=false&description=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=DeBERTa+LGBM+FeatureEngineering学习&body=Check out this article: http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&title=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&name=DeBERTa+LGBM+FeatureEngineering学习&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/05/03/DeBERTa-LGBM-FeatureEngineering%E5%AD%A6%E4%B9%A0/&t=DeBERTa+LGBM+FeatureEngineering学习"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    blacsheep
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
